{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lang_sam import LangSAM\n",
    "import lang_sam.utils as lsu\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LangSAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.open(\"/home/regal/devel/ws_cacti/src/hri_cacti_xr/gesture_recognition/gesture_recogition_research/action_clip_data/hl_frames/test/20240131_161505_d_rr_go_right/img_00121.jpg\").convert(\"RGB\")\n",
    "text_prompt = \"human-hand\"\n",
    "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = np.array(image_pil)\n",
    "image = lsu.draw_image(image_rgb, masks, boxes, phrases)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Do not show axes to resemble an image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num frames: '176'\n",
      "frame rate: '10'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regal/miniconda3/envs/lsa/lib/python3.8/site-packages/transformers/modeling_utils.py:874: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/regal/miniconda3/envs/lsa/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "PrintDetails = True\n",
    "# Path = '/home/regal/devel/ws_cacti/src/hri_cacti_xr/data/go_right/30x_4mono_10hz/20240131_162233_d_rr_go_right.avi'\n",
    "Path = '/home/regal/devel/ws_cacti/src/hri_cacti_xr/data/go_right/30x_4mono_10hz/20240131_161505_d_rr_go_right.avi'\n",
    "OutputPath = '/home/regal/devel/ws_cacti/src/hri_cacti_xr/gesture_recognition/gesture_recogition_research/lang-segment-anything/video.mp4'\n",
    "\n",
    "# open video with opencv\n",
    "cap = cv2.VideoCapture(Path)\n",
    "\n",
    "# Get num frames, width, height, and frame rate\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# create writer\n",
    "out = cv2.VideoWriter(OutputPath, cv2.VideoWriter_fourcc(*\"mp4v\"), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "if(PrintDetails): \n",
    "    print(f\"num frames: '{num_frames}'\")\n",
    "    print(f\"frame rate: '{frame_rate}'\")\n",
    "\n",
    "#Loop through each frame in the video\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    for num_frame in range(0, num_frames):\n",
    "        \n",
    "        # read the current frame as a numpy array\n",
    "        ret, raw_frame = cap.read()\n",
    "\n",
    "        # check if correctly returned\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # convert to image to pass into model\n",
    "        pil_image = Image.fromarray(raw_frame)\n",
    "            \n",
    "        # determine depth from model (from run_video.py on GitHub)\n",
    "        masks, boxes, phrases, logits = model.predict(pil_image, \"human arm from an egocentric view\")\n",
    "        image = lsu.draw_image(raw_frame, masks, boxes, phrases)\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        img_resize = cv2.resize(image_bgr, (frame_width, frame_height))\n",
    "        out.write(img_resize)\n",
    "            \n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
